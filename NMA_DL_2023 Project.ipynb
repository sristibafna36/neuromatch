{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mekatebi/NMA_DL_2023_Project/blob/main/NMA_DL_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWSpp6sGNpTb"
      },
      "source": [
        "# Codes for NMA DL 2023 project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGC2EERfzMIo"
      },
      "source": [
        "## Set-up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z-XlhdUTMnFi"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests nlpaug sacremoses datasets transformers[torch] evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VTOA4ih_zUgU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import requests\n",
        "from torch import nn\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Mekatebi/NMA_DL_2023_Project\") # , revision=\"v2.0.0\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"Mekatebi/NMA_DL_2023_Project\", output_attentions=True) # , revision=\"v2.0.0\")"
      ],
      "metadata": {
        "id": "6iJcXwN8IZ7X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwjFpVpqO4Rb"
      },
      "source": [
        "## RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "aYl4L2VuaZVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J7DZXqQzSOY"
      },
      "source": [
        "### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JfzdQlWxue8Q"
      },
      "outputs": [],
      "source": [
        "# Define the remote file to retrieve\n",
        "url = 'https://zenodo.org/record/2667859/files/500_Reddit_users_posts_labels.csv'\n",
        "# Define the local filename to save data\n",
        "local_file = '/content/Dataset.csv'\n",
        "# Make http request for remote file data\n",
        "data = requests.get(url)\n",
        "# Save file data to local copy\n",
        "with open(local_file, 'wb')as file:\n",
        "  file.write(data.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation"
      ],
      "metadata": {
        "id": "L-NyFvZDDcr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as naf\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "from nlpaug.util import Action"
      ],
      "metadata": {
        "id": "jWDOXmGQDZqJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naf.Sequential([\n",
        "    naw.SpellingAug(aug_min=0, aug_max=1024, aug_p=0.04),\n",
        "    naw.SynonymAug(aug_src='wordnet', aug_min=0, aug_max=1024, aug_p=0.04),\n",
        "    naw.RandomWordAug(action='delete', aug_min=0, aug_max=1024, aug_p=0.04),\n",
        "    naw.RandomWordAug(action='swap', aug_min=0, aug_max=1024, aug_p=0.04)\n",
        "])\n",
        "\n",
        "#    naw.BackTranslationAug(\n",
        "#    from_model_name='facebook/wmt19-en-de',\n",
        "#    to_model_name='facebook/wmt19-de-en',\n",
        "#    max_length=512,\n",
        "#    device='cpu')"
      ],
      "metadata": {
        "id": "bzxcVI38DqRr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "szecBTgZgdIq"
      },
      "outputs": [],
      "source": [
        "# Edit the dataset\n",
        "\n",
        "dataset = pd.read_csv(local_file)\n",
        "\n",
        "dataset = dataset[dataset['Label'].isin(['Indicator', 'Ideation', 'Behavior', 'Attempt'])]\n",
        "dataset['Label'] = dataset['Label'].replace('Indicator','Ideation')\n",
        "dataset['Label'] = dataset['Label'].replace('Behavior','Attempt')\n",
        "\n",
        "dataset = dataset.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows = dataset.shape[0]\n",
        "\n",
        "for i in range(0, rows):\n",
        "\n",
        "  dataset.loc[i + rows, ('Post')] = aug.augment(dataset.loc[i, ('Post')])\n",
        "  dataset.loc[i + rows, ('Label')] = dataset.loc[i, ('Label')]\n",
        "\n",
        "rows = dataset.shape[0]\n",
        "\n",
        "for j in range(0, rows):\n",
        "\n",
        "  dataset.loc[j + rows, ('Post')] = aug.augment(dataset.loc[j, ('Post')])\n",
        "  dataset.loc[j + rows, ('Label')] = dataset.loc[j, ('Label')]\n",
        "\n",
        "dataset.to_csv('/content/Modified_Dataset.csv')"
      ],
      "metadata": {
        "id": "VhQgXvc7Ird1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9y4eNabfX39"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('csv', data_files='/content/Modified_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vYooivmgFQ-"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TZkZjvin7Du"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjO7QnLxheON"
      },
      "outputs": [],
      "source": [
        "def not_none(example):\n",
        "    return example['Post'] is not None\n",
        "\n",
        "dataset = dataset.filter(not_none)\n",
        "\n",
        "dataset_sampled = dataset['train'].train_test_split(test_size=0.01, seed=2023)['train']\n",
        "\n",
        "train_val_test = dataset_sampled.train_test_split(test_size=0.3, seed=2023)\n",
        "train_dataset = train_val_test['train']\n",
        "test_val_dataset = train_val_test['test']\n",
        "\n",
        "test_val_split = test_val_dataset.train_test_split(test_size=0.5, seed=2023)\n",
        "validation_dataset = test_val_split['train']\n",
        "test_dataset = test_val_split['test']\n",
        "\n",
        "columns_to_keep = ['Post', 'Label']\n",
        "\n",
        "columns_to_remove = [col for col in dataset_sampled.column_names if col not in columns_to_keep]\n",
        "\n",
        "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
        "validation_dataset = validation_dataset.remove_columns(columns_to_remove)\n",
        "test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "le.fit(dataset_sampled['Label'])\n",
        "\n",
        "def encode_labels(example):\n",
        "    example['Label'] = le.transform([example['Label']])[0]\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(encode_labels)\n",
        "validation_dataset = validation_dataset.map(encode_labels)\n",
        "test_dataset = test_dataset.map(encode_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFeHflYoLJLy"
      },
      "outputs": [],
      "source": [
        "labels= le.classes_\n",
        "\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "\n",
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TL57VMijCVx"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "def prepare_data(example):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        example['Post'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'labels': torch.tensor(example['Label'], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "train_dataset = train_dataset.map(prepare_data)\n",
        "validation_dataset = validation_dataset.map(prepare_data)\n",
        "test_dataset = test_dataset.map(prepare_data)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKiOtKzE1CBq"
      },
      "outputs": [],
      "source": [
        "np.histogram(test_dataset['Label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd65yf1Nn_ue"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxta18szjQIb"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1PuA3N5oIAT"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHRkueUb1xf5"
      },
      "outputs": [],
      "source": [
        "# model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(le.classes_), id2label=id2label, label2id=label2id) # , problem_type=\"multi_label_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZVMjo6zH14Y"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA4IEM082sJJ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "w4ARqPTE26FP"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./Model',\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    push_to_hub=False,\n",
        "    hub_model_id = \"NMA_DL_2023_Project\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ohAFVCppORS-"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # compute custom loss\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0], device=model.device))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RfYt-aCQPaC-"
      },
      "outputs": [],
      "source": [
        "# CustomTrainer for Multiclass\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jObUVOVM5QHR"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f2q6YeE5UVf"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push to Hugging Face\n",
        "\n",
        "tokenizer.push_to_hub(\"NMA_DL_2023_Project\")\n",
        "model.push_to_hub(\"NMA_DL_2023_Project\")"
      ],
      "metadata": {
        "id": "EgMF9uAUgmoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwPnaCDaKqJI"
      },
      "source": [
        "## BertViz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi2tSARAKxhQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertviz\n",
        "\n",
        "from bertviz import head_view, model_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B-bbdDLMjce"
      },
      "source": [
        "### Head View"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Example = \"I tried to kill my self once and failed badly cause in the moment i wanted to do it i realized that i want to live!\""
      ],
      "metadata": {
        "id": "xGTa-D2EkqIJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(Example , return_tensors='pt')\n",
        "outputs = model(inputs)\n",
        "\n",
        "attention = outputs[-1] # Output includes attention weights when output_attentions=True\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[0])"
      ],
      "metadata": {
        "id": "fvFmVuiBiWDb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head_view(attention, tokens)"
      ],
      "metadata": {
        "id": "PQdUxebXiXm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME"
      ],
      "metadata": {
        "id": "TAukQDow0DaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lime"
      ],
      "metadata": {
        "id": "hWRLwRkx0FWJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import lime\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "class_names = ['Attempt', 'Ideation']\n",
        "\n",
        "def predictor(texts):\n",
        "    outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
        "    tensor_logits = outputs[0]\n",
        "    probas = F.softmax(tensor_logits).detach().numpy()\n",
        "    return probas\n",
        "\n",
        "text = \"I tried to kill myself.\"\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "exp = explainer.explain_instance(text, predictor, num_samples=1000)\n",
        "exp.show_in_notebook(text=text)"
      ],
      "metadata": {
        "id": "d1vlK-zj2pKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers Interpret"
      ],
      "metadata": {
        "id": "8LtjiW0k93k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers-interpret"
      ],
      "metadata": {
        "id": "fRiQfMd395ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Example = \"I just took 10 more. Okay I threw up a little bit but now I feel weird and bloated. Its not so much that I want to die but Im scared and I dont see a way out. I dont see the light at the end of the tunnel anymore. Its just black. I feel like my life is hopeless so why prolong the suffering. Is there anyone out there?\""
      ],
      "metadata": {
        "id": "Awy2BElZBtaA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "\n",
        "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "word_attributions = cls_explainer(Example) # , class_name=\"Attempt\")"
      ],
      "metadata": {
        "id": "YL7Ja1yu-qnA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_explainer.predicted_class_name"
      ],
      "metadata": {
        "id": "TIb4vOT8_Ztu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_explainer.visualize()"
      ],
      "metadata": {
        "id": "kg8qiEFD_p2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dGC2EERfzMIo",
        "CwjFpVpqO4Rb",
        "qwPnaCDaKqJI",
        "TAukQDow0DaX",
        "8LtjiW0k93k2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}